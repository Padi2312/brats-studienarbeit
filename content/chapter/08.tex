%!TEX root = ../../main.tex
\chapter{Experimente}
In diesem Kapitel werden verschiedene Experimente durchgeführt, um die Auswirkungen von verschiedenen Parametern und Verarbeitungsschritten zu untersuchen. Insbesondere werden die Lernrate, Batch Größe und die Vorverarbeitung genauer untersucht. Jeder dieser Komponenten spielt eine wesentliche Rolle bei dem Training des \gls{Modell}s. Bei der Untersuchung von Lernrate und Batch Größe werden verschiedene Werte getestet, um zu sehen wie der Trainingsprozess verläuft. Bei der Komponente Vorverarbeitung wird das Thema der Größenskalierung bzw. des Bildausschnitts eine Rolle spielen.\\
Zunächst werden verschiedene Metriken vorgestellt, anhand derer man die Auswirkungen der verschiedenen Parameter beurteilen kann. Anschließend werden die einzelnen Experimente genauer erläutert und deren Ergebnisse präsentiert.

\section{Metriken}
Die Bewertung eines \gls{Modell}s ist ein wichtiger Schritt in der Entwicklung. Mit Metriken wird ein \gls{Modell} beurteilt und es wird die Leistung evaluiert. Es gibt verschieden Methoden bzw. Berechnungen, um die Leistungsfähigkeit zu untersuchen. Es werden nachfolgend verschiedene Metriken aufgezeigt, sowie deren Ergebnisse auf die jeweiligen \gls{Modell}e vorgestellt.
Metriken sind bestimmte Funktionen, anhand derer die Leistung eines \glspl{Modell} bewertet werden kann. Sie berechnen die Übereinstimmung zwischen der Vorhersage und dem tatsächlichen Ergebnis. Abhängig von der Problemstellung sind eine oder mehrere geeignete Metriken auszuwählen. Eine einzelne Metrik reicht oft nicht aus für eine allumfassende Bewertung, deshalb werden meist mehrere Metriken betrachtet.
\subsection{Jaccard-Index}
Der Jaccard-Index, auch Intersection over Union genannt, ist eine Metrik für die Bewertung der Ähnlichkeit oder Überlappung zweier Mengen $A$ und $B$. Der Jaccard-Index misst das Verhältnis der Schnittmenge beider Mengen zur Vereinigung dieser. In Bezug auf Bildsegmentierung bedeutet dies, dass der Jaccard-Index das Verhältnis des Bereichs des gemeinsamen Segments zum Bereich der vereinigten Segmente misst. Der Jaccard-Index kann einen Wert zwischen 0 und 1 haben, wobei 0 für keine Überlappung und 1 für perfekte Übereinstimmung steht. Mathematisch wird der Jaccard-Index wie folgt berechnet: \cite[vgl.][]{GarciaGarcia2017}
\begin{equation}
	J(A,B) =  \dfrac{\vert A \cup B\vert }{\vert A \cap B\vert}
\end{equation} 

\subsection{Dice-Koeffizient}
\label{subsec:DiceKoeffizient}
Der Dice-Koeffizient, auch als Sørensen-Dice-Koeffizient oder F1-Score bekannt, misst ebenfalls die Ähnlichkeit zwischen zwei Mengen oder Segmentierungen $A$ und $B$. Der Dice-Koeffizient berechnet das Verhältnis des doppelten der Schnittmenge zur Summe der Größe beider Mengen. In Bezug auf Bildsegmentierung misst der Dice-Koeffizient das Verhältnis der doppelten Fläche des gemeinsamen Segments zur Summe der Flächen beider Segmente. Wie der Jaccard-Index kann auch der Dice-Koeffizient Werte zwischen 0 und 1 annehmen, wobei 0 für keine Überlappung und 1 für perfekte Übereinstimmung steht. Mathematisch wird der Dice-Koeffizient wie folgt berechnet:
\begin{equation}
	DSC(A,B) =  \dfrac{2 \vert A \cup B\vert }{\vert A \vert + \vert B\vert}
\end{equation} 
Alternativ kann der Dice-Koeffizient auch mittels boolschen Daten dargestellt werden:
\begin{equation}
	DSC={\frac {2TP}{2TP+FP+FN}}
\end{equation}
''True Positive`` (TP) enthält die Anzahl der korrekt als positiv segmentierten Pixel, die tatsächlich zur gewünschten Klasse gehören. ''False Positive``(FP) gibt die Anzahl der fälschlicherweise als positiv segmentierten Pixel, die tatsächlich nicht zur gewünschten Klasse gehören. ``False Negative'' (FP) hingegen beinhaltet die Anzahl der Pixel die tatsächlich zur gewünschten Klasse gehören, aber fälschlicherweise als negativ segmentiert wurden. \cite[vgl.][]{Dice1945}


\subsection{Pixel Accuracy}
Pixelgenauigkeit, auch als Pixel Accuracy bezeichnet, ist eine Bewertungsmetrik, die häufig im Bereich der Computer Vision und der Bildsegmentierung verwendet wird. Sie misst den Prozentsatz der korrekt klassifizierten Pixel in einem Bild oder einer Gruppe von Bildern.
Die Pixelgenauigkeit wird berechnet, indem die vorhergesagten Labels jedes Pixels im Bild mit den Ground-Truth-Labels verglichen werden. Wenn das vorhergesagte Label mit dem Ground-Truth-Label für einen Pixel übereinstimmt, wird dies als korrekte Klassifikation betrachtet. Die Pixelgenauigkeit wird dann bestimmt, indem die Gesamtzahl der korrekt klassifizierten Pixel durch die Gesamtzahl der Pixel im Bild geteilt wird:
\begin{equation}
	PA = \dfrac{\sum_{i=0}^{k} p_{ii}}{\sum_{i=0}^{k} \sum_{j=0}^{k} p_{ij}}
\end{equation}
Hierbei sind alle $p_{ii}$ als True Positive anzusehen und alle $p_{ij}$ als False Positive bzw. False Negative. \cite[vgl.][]{Hurtado2022}

\section{Beschreibung Experimente}
Nachfolgend werden die verschiedenen Experimente bzw. \glspl{Modell} beschrieben, sowie die Rahmenbedingungen festgelegt. Der verwendete Datensatz enthält 25000 2D Bilder mit einer Größe von je 128x128 Pixeln. Die Vorverarbeitung beinhaltet das zusammenführen der vier Modalitäten, sowie ein Zuschneiden und Skalieren der einzelnen Bilder. Als Standardwerte für die jeweils nicht zu untersuchenden Parameter werden die Werte aus Tabelle \ref{table:StandardWerte} festgelegt.
\begin{table}[!ht]
\begin{longtable}{|c|c|}
	\hline
		\multicolumn{1}{|c|}{\textbf{Komponente}} & \multicolumn{1}{c|}{\textbf{Wert}} \\
		\endhead
	\hline
		Batch Größe & 32 \\
	\hline
		Lernrate & 0.01 \\
	\hline
		Anzahl der Filter & 64, 128, 256, 512 \\
	\hline
		Vorverarbeitung & Auschnitt und Skalierung des Bildes \\
	\hline
		Optimierer & Adam \\
	\hline
		Verlustfunktion & Dice Loss \\
	\hline
		Epochen & 20 \\
	\hline
\end{longtable}
\caption{Standardwerte für das Training des Neuronalen Netzes}
\label{table:StandardWerte}
\end{table}

Die Auswertung der \glspl{Modell} erfolgt über die drei genannten Evaluierungsfunktionen. Die Ausgaben des Neuronalen Netzes werden zunächst mit Hilfe der Softmax-Funktion in den Wertebereich zwischen 0 und 1 überführt. Die neuen Werte stellen dabei eine Wahrscheinlichkeitsverteilung dar, wie bereits in Abschnitt \ref{subsec:AktivierungsfunktionenVerlust-FunktionenOptimierer} erwähnt. Anschließend werden diese Werte mit der Argmax-Funktion\footnote{Identifiziert die Kategorie mit dem höchsten Wert und gibt somit die Klassifizierung an} in die eigentlichen Tumorklassen überführt.

\subsection{1. Experiment - Lernrate}
Der erste zu untersuchende Parameter ist die Lernrate. Sie bestimmt wie stark die Gewichte innerhalb des \gls{Modell}s angepasst werden. Eine sehr hohe Lernrate kann dazu führen, dass wichtige Minima der Verlustfunktion übersprungen werden, während eine zu kleine Lernrate nur sehr langsam konvergiert. Im Folgenden wurde das Neuronale Netz mit verschiedenen Lernraten trainiert, wobei sowohl das Training als auch die abschließende Evaluation betrachtet. Die Lernrate wurde in diesem Versuch, ausgehend vom Standardwert $0.01$, jeweils einmal um den Faktor $1\cdot 10^{-1}$ verkleinert und vergrößert, so dass für das Experiment die Lernraten $0.1$ und $0.001$ ergeben.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Dice & Jaccard Index & Pixel Accuracy \\
		\hline
		LR0.1	& 81,49 			& 79,45 			& 98,79 \\
		\hline
		Standard& 88,44 			& 85,85 			& 99,22  \\
		\hline
		\textbf{LR0.001}	& \textbf{90,32} 	&  \textbf{87,81}	& \textbf{99,33} \\
		\hline
	\end{tabular}
	\caption{Ergebnisse des 1. Experiments mit der Lernrate in Prozent (Quelle: Eigene Darstellung)}
		\label{table:1.ExperimentErgebnisse}
\end{table}

\subsection{2. Experiment - Batch Größe}
Der zweite Parameter ist die Batch Größe, welche angibt nach wie vielen Trainingsbeispielen die internen Parameter angepasst werden. Es wird untersucht, wie sich das Trainingsverhalten und die endgültige Leistung des \gls{Modell}s in Abhängigkeit der Batch Größe unterscheiden. Die zu untersuchenden Werte ergeben sich aus dem Standardwert, indem dieser einmal halbiert und einmal verdoppelt wird. Daraus ergeben sich somit die Batch Größen von $8$ und $64$.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Dice & Jaccard Index & Pixel Accuracy \\
		\hline
		B8					& 83,03 			&81,08  			& 98,88 \\
		\hline
		\textbf{Standard}	& \textbf{88,44} 	& \textbf{85,85}  	& \textbf{99,22}  \\
		\hline
		B64					& 85,28  			& 82,66 			& 99,04 \\
		\hline
	\end{tabular}
	\caption{Ergebnisse des 2. Experiments mit der Batch Größe in Prozent (Quelle: Eigene Darstellung)}
	\label{table:2.ExperimentErgebnisse}
\end{table}

\subsection{3. Experiment - Vorverarbeitung}
\label{subsec:3.Experiment-Vorverarbeitung}
Das letzte Experiment untersucht keinen Hyperparameter, sondern die Auswirkungen der Vorverarbeitungen auf die Leistung des \gls{Modell}s. Wie bereits in Abschnitt \ref{paragraph:GrößenSkalierung} erwähnt, müssen die Eingaben für das U-Net eine bestimmte Größe aufweisen, damit diese verarbeitet werden können. Die Größe der Bilder kann auf unterschiedliche Weise in die gewünschte Eingabegröße gebracht werden. Eine der Methoden ist eine einfache Skalierung von der Originalgröße auf die Zielgröße, dabei kommt es zu Verlusten der Bildqualität, da umliegende Pixel bzw. dessen Farbwerte miteinander verrechnet werden. Die zweite Methode, welche genauer in Abschnitt \ref{paragraph:GrößenSkalierung} beschrieben wurde, besteht darin zunächst die nicht relevanten Teile des Bildes zu entfernen und anschließend auf die gewünschte Zielgröße zu skalieren. Durch diese Methode enthält das Bild weniger irrelevante Informationen und erfährt einen geringeren Qualitätsverlust. 
\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Dice & Jaccard Index & Pixel Accuracy \\
		\hline
		\textbf{Standard}& \textbf{88,44} 	& \textbf{85,85}  	& \textbf{99,22}  \\
		\hline
		Scaled		& 81,43  	& 79,22 	& 98,80 \\
		\hline
	\end{tabular}
	\caption{Ergebnisse des 2. Experiments mit der Batch Größe in Prozent (Quelle: Eigene Darstellung)}
	\label{table:3.ExperimentErgebnisse}
\end{table}

\section{Diskussion der Ergebnisse}
Im folgenden Abschnitt werden die Ergebnisse der Experiment genauer betrachtet. Sowohl wie Trainingsverläufe, als auch die Resultate der Evaluation des \gls{Modell}s, über den Testdatensatz, werden genauer beschrieben. Bei den Verlustkurven stellt die blaue Kurve immer das Referenzmodell mit den Standardwerten dar. Es wird dabei außerdem auf die Auswirkungen der verschiedenen Parameter, so wie die Aussagekraft der Metriken eingegangen. 

\subsection{1. Experiment}
\label{subsec:1.Experiment}
Das erste Experiment untersuchte die Lernrate im Hinblick auf das Training und die Leistung des \gls{Modell}s. Zuerst wird der Verlauf des Trainings betrachtet, welcher in Anhang \ref{appendix:Experiment1Verlustkurven} zu sehen ist, anschließend werden die verschiedenen Metriken hinzugezogen, um die Leistung der Modelle zu vergleichen. \\
Zunächst wird der Verlust mit Trainings- und Validierungsdatensatz betrachtet und interpretiert. Die Abbildungen \ref{fig:ex1_lr_train} und \ref{fig:ex1_lr_val} zeigen die Verlustkurven der verschiedenen Modell mit je dem Trainings- und Validierungsdatensatz.\\ 
Die nachfolgende Diskussion und Interpretation der Kurven beziehen sich vorerst, sofern nicht anders erwähnt, auf die Abb. \ref{fig:ex1_lr_train} und den Trainingsdatensatz. Die grüne Kurve, welche das Modell ``LR0.1'' mit der Lernrate $0.1$ beschreibt, hat direkt von Anfang an einen geringeren Verlust als das Referenzmodell, aber verringert diesen über die Zeit nur noch minimal. Möglicherweise werden hier wichtige Minima der Verlustfunktion übersprungen, weshalb das  Modell ``LR0.1'' relativ schnell mit einem hohen Verlust im konvergiert.\\
Vergleichsweise dazu hat das Modell ``LR0.001'', die rote Kurve, mit einer Lernrate von $0.001$, zu Beginn einen höheren Verlust, welcher noch über einige Epochen so bleibt und dann schlagartig fällt. Der Verlust von Modell ``LR0.001'' unterschreitet den des Referenzmodells und hat am Ende der 20 Epochen einen geringeren Verlust als das Referenzmodell. Durch die geringe Lernrate von Modell ``LR0.001'' sinkt der Verlust zu Beginn vergleichsweise langsam, da die internen Parameter nur minimal angepasst werden. Auf Dauer führen diese kleinen Schritte jedoch zu einem besseren Ergebnis, da wichtige Minima der Verlustfunktion weniger oder gar nicht übersprungen werden und so das \gls{Modell} besser trainiert wird. Diese Beobachtungen treffen so auch auf den Validieriungsdatensatz zu und können ähnlich interpretiert werden. Die Kurven in Abb. \ref{fig:ex1_lr_val} sind ähnlich zu den Kurven in \ref{fig:ex1_lr_train}. Hier wird ebenfalls deutlich, dass eine zu hohe Lernrate keine optimalen Resultate erbringt. So konvergiert der Verlust von Modell ``LR0.1'' bereits ab der 14. Epoche mit einem relativ hohen Wert von etwa $0.25$, während das Modell ''LR0.001`` und das Referenzmodell noch weiter den Verlust verringern.

Die Ergebnisse der Metriken (siehe Tabelle \ref{table:1.ExperimentErgebnisse}) unterstreichen den Verlauf des Trainings, so hat Modell ``LR0.001'' die beste Leistung erbracht. Ein Blick auf die Metriken ergibt einen Dice Koeeffizenten von über 90\% und einen Jaccard Index von rund 88\%, was recht gute Ergebnisse sind im Anbetracht des kurzen Trainings und der geringen Vorverarbeitung. Die Pixel Accuracy hingegen ist unaussagekräftig, da sich die Werte nur minimal unterscheiden mit einer Differenz von maximal $0.54\%$.

\subsection{2. Experiment}
Im Rahmen des zweiten Experiments wurde die Auswirkung der Batch-Größe untersucht. Zunächst werden, ähnlich wie im ersten Experiment (siehe Abschnitt \ref{subsec:1.Experiment}), die Trainingsverläufe betrachtet (siehe Anhang \ref{appendix:Experiment2Verlustkurven}), gefolgt von einer Bewertung der Leistung anhand der genannten Metriken.\\
Zu Beginn wird der Verlust mit dem Trainingsdatensatz betrachtet, welcher in Abbildung \ref{fig:ex2_batch_train} dargestellt ist. Die Verlustkurven verlaufen in diesem Experiment sehr ähnlich, jedoch mit unterschiedlichen Verlustwerten. Die grüne Kurve, das Modell ``B64'' mit einer Batch Größe von $64$, hat im Vergleich zum Referenzmodell einen höheren Verlust in den Anfangsepochen, nähert sich in den Endepochen jedoch dem Referenzmodell an. Die rote Kurve, das Modell ``B8'' mit einer Batchgröße von $8$, hat einen hohen Verlust im Vergleich zu den anderen beiden Modellen. \\
Ein interessanter Aspekt ist der Verlauf der Verlustkurven in Bezug auf den Validierungsdatensatz, der in Abbildung \ref{fig:ex2_batch_val} dargestellt ist, welcher nachfolgend genauer betrachtet wird. Dabei fällt auf, dass die rote Kurve von Beginn an einen niedrigen Validierungsverlust hat, aber eher unruhig verläuft und häufiger steile Anstiege aufweist. Im Gegensatz dazu verläuft die grüne Kurve etwas gleichmäßiger weißt jedoch auch einige Anstiege auf. Allerdings steigt die grüne Kurve gegen Ende des Trainings wieder an, ab der 16. Epoche zeigt das Modell ``B64'' einen kontinuierlichen Anstieg der Verlustwerte, was auf ein Overfitting des Modells hinweisen könnte. Das Referenzmodell mit der blauen Kurve hingegen weist einen ruhigeren Verlauf auf, zeigt jedoch auch einen signifikanten Sprung ab etwa der Hälfte des Verlaufes. Es zeigt sich, dass das Modell ``B64'' anfänglich eine bessere Performance aufweist, aber später ein Overfitting entwickelt. Das Referenzmodell hingegen zeigt insgesamt eine stabilere Leistung, obwohl es im Bezug auf den Validierungsdatensatz einen deutlichen Sprung des Verlustwerts aufweist. 

Bei Betrachten der Metriken in Tabelle \ref{table:2.ExperimentErgebnisse} wird deutlich, dass das Referenzmodell im Vergleich zu den anderen beiden Modellen die besten Ergebnisse erzielt. Sowohl der Dice Koeffizienten, als auch der Jaccard Index weisen eine Verbesserung von rund $3\%$ für das Referenzmodell gegenüber dem Modell ``B64'' auf. Die Pixel Accuracy liefert in diesem Fall keine aussagekräftigen Informationen über die Leistung der Modelle, da die Unterschiede nur minimal sind. Die Wahl der Batch Größe von 32 bei einer Datenmenge von 25000 Bildern erweist sich somit als geeignet und führt zu guten Ergebnissen.


\subsection{3. Experiment}
\label{subsec:3.Experiment}
Das dritte Experiment untersucht die Auswirkungen der Vorverarbeitung auf die Qualität und Leistung des Modells. Es sei nochmals darauf hingewiesen, dass der Unterschied zwischen den beiden Modellen lediglich in einem Schritt der Vorverarbeitung besteht, wie bereits in Abschnitt \ref{subsec:3.Experiment-Vorverarbeitung} erläutert. Die verwendeten Hyperparameter für dieses Experiment entsprechen den Standardwerten gemäß Tabelle \ref{table:StandardWerte}. Ebenso werden zunächst die Trainingsverläufe betrachtet und abschließend die Leistung der Modelle mithilfe der genannten Metriken bewertet. Die Verlustkurven für dieses Experiment sind in Anhang \ref{appendix:Experiment3Verlustkurven} dargestellt.\\
Der Verlauf des Verlusts im Bezug auf den Trainingsdatensatz, dargestellt in Abb. \ref{fig:ex3_prepro_train}, ist bei beiden Modellen recht ähnlich. Die rote Kurve, Modell ``Scaled'', besitzt, bis auf zu Beginn, immer einen höheren Verlust als das Referenzmodell. Der Verlauf beider kurven ist nahezu identisch, jedoch differieren die Werte der beiden Kurven um etwa $0.05 - 0.1$. Die Ähnlichkeit der beiden Kurven zeigt, dass sich die Vorverarbeitung, in diesem Fall das entfernen irrelevanter Informationen, positiv auf die Leistung auswirkt, aber keinen größeren Einfluss auf den Verlauf des Trainings besitzt.\\
Parallelen der beiden Modelle finden sich auf im Verlauf des Verlust im Bezug auf den Validierungsdatensatz wieder, welcher in Abb. \ref{fig:ex3_prepro_val} zu sehen ist. Besonders am Anfang und Ende des Trainings verlaufen die beiden Kurven sehr analog zueinander, lediglich die rote Kurve hat höhere Werte wie auch im Trainingsdatensatz. Hier zeigt sich ebenfalls, dass durch geeignete Vorverarbeitungsmaßnahmen bessere Leistungen während des Trainings erzielt werden können.

Deutlich sieht man diese Folgerung auch anhand der Metriken in Tabelle \ref{table:3.ExperimentErgebnisse}. Dort hat das Referenzmodell einen deutlich höheren Dice Koeffizienten, sowie auch Jaccard Index. Die Differenz liegt in diesem Fall nicht nur bei einer geringen Prozentzahl wie bei den vorherigen Experiment, sondern bei etwa $7\%$, was ein deutlicher Unterschied in Sachen Leistung ist.


\section{Fehleranalyse und Verbesserungen}
Im Hinblick auf die Weiterentwicklung dieses Modells gilt es potentielle Fehlerquellen zu beseitigen und Verbesserungen bezüglich seiner Leistung auszuarbeiten. Die Tatsache, dass jedes der Modelle einen gewissen ``Grundverlust'' aufweist, lässt darauf schließen, dass es noch Raum für Verbesserungen gibt. Ein wichtiger Aspekt, der bereits in Kapitel \ref{sec:Datensatz} behandelt wurde, ist die Vorverarbeitung der Daten. Verschiedene Faktoren wie Skalierung, Normalisierung und Datenaugmentierung können sich auf die Leistung des Modells auswirken.\\
Die Ergebnisse des dritten Experiments in Abschnitt \ref{subsec:3.Experiment} verdeutlichen die Bedeutung der Entfernung irrelevanter Informationen aus den Bildern. Die Vorverarbeitungsmethoden, die in dieser Arbeit angewendet wurden, könnten weiter verbessert werden, indem die Daten besser zugeschnitten werden, um möglichst viele relevante Informationen dem neuronalen Netzwerk zuzuführen. Zudem sollte eine höhere Auflösung der Bilder in Betracht gezogen werden, da die Skalierung zu Qualitätsverlusten führt und das Modell aufgrund weniger verfügbarer Pixel beeinträchtigt.\\
Im Rahmen der Experimente wurden lediglich 25.000 Bilder verwendet. Eine deutliche Steigerung dieser Anzahl ermöglicht es dem Modell, mehr Bilder für das Training zu nutzen und somit mehr Strukturen zu erlernen. Aufgrund begrenzter Ressourcen und zeitlicher Einschränkungen wurde jedoch nur eine Teilmenge der verfügbaren Daten verwendet. Wenn die gesamte Datenmenge von etwa 160.000 Bildern für das Training herangezogen wird, sollte auch die Anzahl der Schichten im neuronalen Netzwerk erhöht werden. Durch eine größere Anzahl von Schichten kann das Netzwerk mehr Feature-Maps generieren, die für die Segmentierung von Gehirntumoren verwendet werden können. \cite[vgl.][]{Teoh2023}\\
Ebenfalls eine Verbesserung der Leistung des Modells kann durch die optimierte Auswahl der Hyperparameter kommen. Es kann nicht nur zur Leistungssteigerung führen, sondern auch zu einem effektiveren Training. kann ebenfalls zu einer Steigerung der Leistung des Modells, sowie einem effizienteren Training führen. Hierfür kann man systematisch diverse Kombinationen von Hyperparametern auswählen. 


