%!TEX root = ../../main.tex
\chapter{Experimente}
In diesem Kapitel werden verschiedene Experimente durchgeführt, um die Auswirkungen verschiedener Parameter und Verarbeitungsschritte zu untersuchen. Insbesondere werden die Lernrate, die Losgröße und die Vorverarbeitung näher betrachtet. Jede dieser Komponenten spielt eine wesentliche Rolle beim Training des \gls{Modell}s. Bei der Untersuchung der Lernrate und der Losgröße werden verschiedene Werte getestet, um zu sehen, wie der Trainingsprozess abläuft. In der Vorverarbeitungskomponente wird das Thema Skalierung bzw. Bildausschnitt eine Rolle spielen.\\
Zunächst werden verschiedene Metriken vorgestellt, anhand derer die Auswirkungen der verschiedenen Parameter beurteilt werden können. Anschließend werden die einzelnen Experimente näher erläutert und deren Ergebnisse vorgestellt.

\section{Metriken}
Die Bewertung eines \gls{Modell}s ist ein wichtiger Schritt in der Entwicklung. Metriken werden verwendet, um ein \gls{Modell} zu bewerten und seine Leistung zu beurteilen. Es gibt verschiedene Methoden bzw. Berechnungen, um die Performance zu untersuchen. Im Folgenden werden verschiedene Metriken und ihre Ergebnisse für die jeweiligen \gls{Modell}e vorgestellt.
Metriken sind bestimmte Funktionen, mit deren Hilfe die Leistungsfähigkeit eines \glspl{Modell}s bewertet werden kann. Sie berechnen die Übereinstimmung zwischen Vorhersage und tatsächlichem Ergebnis. Je nach Problemstellung müssen eine oder mehrere geeignete Metriken ausgewählt werden. Oft reicht eine einzelne Metrik für eine umfassende Bewertung nicht aus, weshalb in der Regel mehrere Metriken betrachtet werden.
\subsection{Jaccard-Index}
Der Jaccard-Index, auch Intersection over Union genannt, ist eine Metrik zur Bewertung der Ähnlichkeit oder Überlappung zweier Mengen $A$ und $B$. Der Jaccard-Index misst das Verhältnis der Schnittmenge der beiden Mengen zu ihrer Vereinigung. In Bezug auf die Bildsegmentierung bedeutet dies, dass der Jaccard-Index das Verhältnis der Fläche des gemeinsamen Segments zur Fläche der vereinigten Segmente misst. Der Jaccard-Index kann einen Wert zwischen 0 und 1 annehmen, wobei 0 für keine Überlappung und 1 für perfekte Übereinstimmung steht. Mathematisch wird der Jaccard-Index folgendermaßen berechnet: \cite[vgl.][]{GarciaGarcia2017}
\begin{equation}
	J(A,B) =  \dfrac{\vert A \cup B\vert }{\vert A \cap B\vert}
\end{equation} 

\subsection{Dice-Koeffizient}
\label{subsec:DiceKoeffizient}
Der Dice-Koeffizient, auch bekannt als Sørensen-Dice-Koeffizient oder F1-Score, misst ebenfalls die Ähnlichkeit zwischen zwei Mengen oder Segmentierungen $A$ und $B$. Der Dice-Koeffizient berechnet das Verhältnis zwischen dem Doppelten der Schnittmenge und der Summe der Größe der beiden Mengen. In Bezug auf die Bildsegmentierung misst der Dice-Koeffizient das Verhältnis der doppelten Fläche des gemeinsamen Segments zur Summe der Flächen beider Segmente. Wie der Jaccard-Index kann der Dice-Koeffizient Werte zwischen 0 und 1 annehmen, wobei 0 für keine Überlappung und 1 für perfekte Übereinstimmung steht. Mathematisch wird der Dice-Koeffizient wie folgt berechnet
\begin{equation}
	DSC(A,B) =  \dfrac{2 \vert A \cup B\vert }{\vert A \vert + \vert B\vert}
\end{equation} 
Alternativ kann der Dice-Koeffizient auch mittels boolschen Daten dargestellt werden:
\begin{equation}
	DSC={\frac {2TP}{2TP+FP+FN}}
\end{equation}
''True Positive`` (TP) enthält die Anzahl der korrekt als positiv segmentierten Pixel, die tatsächlich zur gewünschten Klasse gehören. ''False Positive``(FP) gibt die Anzahl der fälschlicherweise als positiv segmentierten Pixel, die tatsächlich nicht zur gewünschten Klasse gehören. ``False Negative'' (FP) hingegen beinhaltet die Anzahl der Pixel an, die tatsächlich zur gewünschten Klasse gehören, aber fälschlicherweise als negativ segmentiert wurden. \cite[vgl.][]{Dice1945}


\subsection{Pixel Accuracy}
Die Pixelgenauigkeit, auch Pixel Accuracy genannt, ist eine in der Bildverarbeitung und Bildsegmentierung häufig verwendete Bewertungsmetrik. Sie misst den Prozentsatz der richtig klassifizierten Pixel in einem Bild oder einer Gruppe von Bildern.
Die Pixelgenauigkeit wird berechnet, indem die vorhergesagten Labels jedes Pixels im Bild mit den Ground-Truth-Labels verglichen werden. Wenn das vorhergesagte Label mit dem Ground-Truth-Label für ein Pixel übereinstimmt, wird dies als korrekte Klassifikation betrachtet. Die Pixelgenauigkeit wird dann bestimmt, indem die Gesamtzahl der korrekt klassifizierten Pixel durch die Gesamtzahl der Pixel im Bild dividiert wird:
\begin{equation}
	PA = \dfrac{\sum_{i=0}^{k} p_{ii}}{\sum_{i=0}^{k} \sum_{j=0}^{k} p_{ij}}
\end{equation}
Hierbei sind alle $p_{ii}$ als True Positive anzusehen und alle $p_{ij}$ als False Positive bzw. False Negative. \cite[vgl.][]{Hurtado2022}

\section{Beschreibung Experimente}
Im Folgenden werden die verschiedenen Experimente bzw. \glspl{Modell} beschrieben und die Rahmenbedingungen festgelegt. Der verwendete Datensatz besteht aus 25000 2D-Bildern mit einer Größe von jeweils 128x128 Pixeln. Die Vorverarbeitung umfasst das Zusammenfügen der vier Modalitäten sowie das Beschneiden und Skalieren der Einzelbilder. Als Standardwerte für die jeweils nicht zu untersuchenden Parameter werden die Werte aus der Tabelle \ref{table:StandardWerte} festgelegt.
\begin{table}[!ht]
\begin{longtable}{|c|c|}
	\hline
		\multicolumn{1}{|c|}{\textbf{Komponente}} & \multicolumn{1}{c|}{\textbf{Wert}} \\
		\endhead
	\hline
		Batch Größe & 32 \\
	\hline
		Lernrate & 0.01 \\
	\hline
		Anzahl der Filter & 64, 128, 256, 512 \\
	\hline
		Vorverarbeitung & Auschnitt und Skalierung des Bildes \\
	\hline
		Optimierer & Adam \\
	\hline
		Verlustfunktion & Dice Loss \\
	\hline
		Epochen & 20 \\
	\hline
\end{longtable}
\caption{Standardwerte für das Training des Neuronalen Netzes}
\label{table:StandardWerte}
\end{table}

Die Auswertung der \glspl{Modell} erfolgt über die drei genannten Evaluierungsfunktionen. Die Ausgänge des Neuronalen Netzes werden zunächst mit Hilfe der Softmax-Funktion in den Wertebereich zwischen 0 und 1 transformiert. Die neuen Werte stellen, wie bereits im Abschnitt \ref{subsec:AktivierungsfunktionenVerlust-FunktionenOptimierer} erwähnt, eine Wahrscheinlichkeitsverteilung dar. Anschließend werden diese Werte mit Hilfe der Argmax-Funktion\footnote{Identifiziert die Kategorie mit dem höchsten Wert und gibt damit die Klassifikation an} in die eigentlichen Tumorklassen überführt.

\subsection{1. Experiment - Lernrate}
Der erste zu untersuchende Parameter ist die Lernrate. Sie bestimmt den Grad der Anpassung der Gewichte innerhalb des \gls{Modell}s. Eine sehr hohe Lernrate kann dazu führen, dass wichtige Minima der Verlustfunktion übersprungen werden, während eine zu niedrige Lernrate nur sehr langsam konvergiert. Im Folgenden wurde das Neuronale Netz mit verschiedenen Lernraten trainiert, wobei sowohl das Training als auch die abschließende Bewertung betrachtet wurden. Ausgehend vom Standardwert $0.01$ wurde die Lernrate jeweils einmal um den Faktor $1\cdot 10^{-1}$ verringert und erhöht, so dass sich für das Experiment Lernraten von $0.1$ und $0.001$ ergeben.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Dice & Jaccard Index & Pixel Accuracy \\
		\hline
		LR0.1	& 81,49 			& 79,45 			& 98,79 \\
		\hline
		Standard& 88,44 			& 85,85 			& 99,22  \\
		\hline
		\textbf{LR0.001}	& \textbf{90,32} 	&  \textbf{87,81}	& \textbf{99,33} \\
		\hline
	\end{tabular}
	\caption{Ergebnisse des 1. Experiments mit der Lernrate in Prozent (Quelle: Eigene Darstellung)}
		\label{table:1.ExperimentErgebnisse}
\end{table}

\subsection{2. Experiment - Batch Größe}
Der zweite Parameter ist die Batch Größe, welche angibt nach wie vielen Trainingsbeispielen die internen Parameter angepasst werden. Es wird untersucht, wie sich das Trainingsverhalten und die endgültige Leistung des \gls{Modell}s in Abhängigkeit der Batch Größe unterscheiden. Die zu untersuchenden Werte ergeben sich aus dem Standardwert, indem dieser einmal halbiert und einmal verdoppelt wird. Daraus ergeben sich somit die Batch Größen von $8$ und $64$.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Dice & Jaccard Index & Pixel Accuracy \\
		\hline
		B8					& 83,03 			&81,08  			& 98,88 \\
		\hline
		\textbf{Standard}	& \textbf{88,44} 	& \textbf{85,85}  	& \textbf{99,22}  \\
		\hline
		B64					& 85,28  			& 82,66 			& 99,04 \\
		\hline
	\end{tabular}
	\caption{Ergebnisse des 2. Experiments mit der Batch Größe in Prozent (Quelle: Eigene Darstellung)}
	\label{table:2.ExperimentErgebnisse}
\end{table}

\subsection{3. Experiment - Vorverarbeitung}
\label{subsec:3.Experiment-Vorverarbeitung}
Das letzte Experiment untersucht keinen Hyperparameter, sondern die Auswirkungen der Vorverarbeitung auf die Performance des \gls{Modell}s. Wie bereits im Abschnitt \ref{paragraph:GrößenSkalierung} erwähnt, müssen die Eingaben für das U-Net eine bestimmte Größe haben, um verarbeitet werden zu können. Die Größe der Bilder kann auf verschiedene Weise auf die gewünschte Eingabegröße gebracht werden. Die eine Methode ist eine einfache Skalierung von der Originalgröße auf die Zielgröße, wobei es zu Einbußen in der Bildqualität kommt, da benachbarte Pixel bzw. deren Farbwerte miteinander verrechnet werden. Die zweite Methode, die im Abschnitt \ref{paragraph:GrößenSkalierung} genauer beschrieben wird, besteht darin, zunächst die irrelevanten Teile des Bildes zu entfernen und dann auf die gewünschte Zielgröße zu skalieren. Bei dieser Methode enthält das Bild weniger irrelevante Informationen und erfährt einen geringeren Qualitätsverlust. 
\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Dice & Jaccard Index & Pixel Accuracy \\
		\hline
		\textbf{Standard}& \textbf{88,44} 	& \textbf{85,85}  	& \textbf{99,22}  \\
		\hline
		Scaled		& 81,43  	& 79,22 	& 98,80 \\
		\hline
	\end{tabular}
	\caption{Ergebnisse des 2. Experiments mit der Batch Größe in Prozent (Quelle: Eigene Darstellung)}
	\label{table:3.ExperimentErgebnisse}
\end{table}

\section{Diskussion der Ergebnisse}
Im folgenden Abschnitt werden die Ergebnisse des Experiments näher betrachtet. Sowohl die Trainingsverläufe als auch die Ergebnisse der Evaluierung des \gls{Modell}s, anhand des Testdatensatzes, werden näher beschrieben. Bei den Verlustkurven stellt die blaue Kurve immer das Referenzmodell mit den Standardwerten dar. Es wird auch auf die Auswirkungen der verschiedenen Parameter sowie auf die Aussagekraft der Metriken eingegangen. 

\subsection{1. Experiment}
\label{subsec:1.Experiment}
Im ersten Experiment wurde die Lernrate in Bezug auf das Training und die Leistung des \gls{Modell}s untersucht. Zuerst wird der Trainingsverlauf betrachtet, der im Anhang \ref{appendix:Experiment1Verlustkurven} zu sehen ist, dann werden die verschiedenen Metriken herangezogen, um die Leistung der Modelle zu vergleichen. \\
Zunächst wird der Verlust mit Trainings- und Validierungsdatensatz betrachtet und interpretiert. Die Abbildungen \ref{fig:ex1_lr_train} und \ref{fig:ex1_lr_val} zeigen die Verlustkurven der verschiedenen Modelle mit dem Trainings- bzw. Validierungsdatensatz.\\ 
Die folgende Diskussion und Interpretation der Kurven bezieht sich zunächst, wenn nicht anders angegeben, auf die Abbildung. \ref{fig:ex1_lr_train} und den Trainingsdatensatz. Die grüne Kurve, die das Modell ``LR0.1'' mit der Lernrate $0.1$ beschreibt, hat von Anfang an einen geringeren Verlust als das Referenzmodell, reduziert diesen aber über die Zeit nur minimal. Möglicherweise werden hier wichtige Minima der Verlustfunktion übersprungen, weshalb das Modell ``LR0.1'' relativ schnell mit einem hohen Verlust im konvergiert.\\
Im Vergleich dazu hat das Modell ``LR0.001'', die rote Kurve, mit einer Lernrate von $0.001$, zu Beginn einen höheren Verlust, der über einige Epochen so bleibt und dann schlagartig abfällt. Der Verlust des Modells ``LR0.001'' ist geringer als der des Referenzmodells und hat am Ende der 20 Epochen einen geringeren Verlust als das Referenzmodell. Aufgrund der geringen Lernrate des Modells ``LR0.001'' sinkt der Verlust zu Beginn vergleichsweise langsam, da die internen Parameter nur minimal angepasst werden. Langfristig führen diese kleinen Schritte jedoch zu einem besseren Ergebnis, da wichtige Minima der Verlustfunktion weniger oder gar nicht übersprungen werden und das \gls{Modell} somit besser trainiert wird. Diese Beobachtungen gelten somit auch für den Validierungsdatensatz und können analog interpretiert werden. Die Kurven in Abb. \ref{fig:ex1_lr_val} sind den Kurven in \ref{fig:ex1_lr_train} ähnlich. Auch hier wird deutlich, dass eine zu hohe Lernrate nicht zu optimalen Ergebnissen führt. So konvergiert der Verlust des Modells ``LR0.1'' bereits ab der 14. Epoche bei einem relativ hohen Wert von ca. $0.25$, während das Modell ``LR0.001'' und das Referenzmodell den Verlust weiter reduzieren.

Die Ergebnisse der Metriken (siehe Tabelle \ref{table:1.ExperimentErgebnisse}) unterstreichen den Verlauf des Trainings, wobei das Modell ``LR0.001'' die beste Performance aufweist. Ein Blick auf die Metriken zeigt einen Dice-Koeffizienten von über 90\% und einen Jaccard-Index von ca. 88\%, was angesichts des kurzen Trainings und der geringen Vorverarbeitung recht gute Ergebnisse sind. Die Pixel Accuracy ist hingegen nicht sehr aussagekräftig, da sich die Werte mit einer maximalen Differenz von $0.54\%$ nur minimal unterscheiden.

\subsection{2. Experiment}
Im zweiten Experiment wurde der Einfluss der Batchgröße Wie im ersten Experiment (siehe Abschnitt \ref{subsec:1.Experiment}) werden zunächst die Trainingskurven (siehe Anhang \ref{appendix:Experiment2Verlustkurven}) betrachtet und anschließend die Performance anhand der genannten Metriken bewertet.\\
Zunächst wird der Verlust mit dem Trainingsdatensatz betrachtet, der in Abbildung \ref{fig:ex2_batch_train} dargestellt ist. Die Verlustkurven verlaufen in diesem Experiment sehr ähnlich, jedoch mit unterschiedlichen Verlustwerten. Die grüne Kurve, das Modell ``B64'' mit einer Batchgröße von $64$, hat im Vergleich zum Referenzmodell einen höheren Verlust in den ersten Epochen, nähert sich aber in den letzten Epochen dem Referenzmodell an. Die rote Kurve, das Modell ``B8'' mit einer Losgröße von $8$, hat im Vergleich zu den beiden anderen Modellen einen hohen Verlust. \\
Ein interessanter Aspekt ist der Verlauf der Verlustkurven in Bezug auf den Validierungsdatensatz, der in Abbildung \ref{fig:ex2_batch_val} dargestellt ist. Es fällt auf, dass die rote Kurve von Anfang an einen geringen Validierungsverlust aufweist, aber einen eher unruhigen Verlauf mit häufigeren steilen Anstiegen hat. Im Gegensatz dazu verläuft die grüne Kurve etwas gleichmäßiger, weist aber auch einige Steigungen auf. Allerdings steigt die grüne Kurve gegen Ende des Trainings wieder an, ab der 16. Epoche zeigt das Modell ``B64'' einen kontinuierlichen Anstieg der Verlustwerte, was auf ein Overfitting des Modells hindeuten könnte. Das Referenzmodell mit der blauen Kurve hat dagegen einen ruhigeren Verlauf, zeigt aber auch einen signifikanten Sprung ab etwa der Hälfte des Verlaufs. Es zeigt sich, dass das Modell ``B64'' zu Beginn eine bessere Performance aufweist, später aber ein Overfitting entwickelt. Das Referenzmodell hingegen zeigt insgesamt eine stabilere Performance, obwohl es einen signifikanten Sprung des Verlustwertes im Vergleich zum Validierungsdatensatz aufweist. 

Bei Betrachtung der Metriken in Tabelle \ref{table:2.ExperimentErgebnisse} wird deutlich, dass das Referenzmodell im Vergleich zu den beiden anderen Modellen die besten Ergebnisse erzielt. Sowohl der Dice Koeffizient als auch der Jaccard Index zeigen eine Verbesserung von ca. $3\%$ für das Referenzmodell gegenüber dem Modell ``B64''. Die Pixelgenauigkeit liefert in diesem Fall keine aussagekräftigen Informationen über die Leistungsfähigkeit der Modelle, da die Unterschiede minimal sind. Die Wahl einer Batchgröße von 32 bei einer Datenmenge von 25000 Bildern erweist sich somit als angemessen und führt zu guten Ergebnissen.

\subsection{3. Experiment}
\label{subsec:3.Experiment}
Das dritte Experiment untersucht die Auswirkungen der Vorverarbeitung auf die Qualität und die Leistung des Modells. Wie bereits im Abschnitt \ref{subsec:3.Experiment-Vorverarbeitung} erläutert, besteht der Unterschied zwischen den beiden Modellen lediglich in einem Vorverarbeitungsschritt. Die für dieses Experiment verwendeten Hyperparameter entsprechen den Standardwerten gemäß Tabelle \ref{table:StandardWerte}. Ebenso werden zunächst die Trainingskurven betrachtet und abschließend die Leistungsfähigkeit der Modelle anhand der genannten Metriken bewertet. Die Verlustkurven für dieses Experiment sind im Anhang \ref{appendix:Experiment3Verlustkurven} dargestellt.\\
Die Verlustkurven für den Trainingsdatensatz sind in Abb. \ref{fig:ex3_prepro_train}, ist für beide Modelle sehr ähnlich. Die rote Kurve, Modell ``Scaled'', hat außer zu Beginn immer einen höheren Verlust als das Referenzmodell. Der Verlauf der beiden Kurven ist nahezu identisch, jedoch unterscheiden sich die Werte der beiden Kurven um ca. $0.05 - 0.1$. Die Ähnlichkeit der beiden Kurven zeigt, dass die Vorverarbeitung, in diesem Fall das Entfernen irrelevanter Informationen, einen positiven Effekt auf die Leistung hat, jedoch keinen größeren Einfluss auf den Trainingsverlauf hat.\\\
Parallelen zwischen den beiden Modellen finden sich im Verlustverlauf in Bezug auf den Validierungsdatensatz, der in Abbildung 3 dargestellt ist. \ref{fig:ex3_prepro_val} zu sehen ist. Insbesondere zu Beginn und am Ende des Trainings verlaufen die beiden Kurven sehr analog zueinander, lediglich die rote Kurve weist wie im Trainingsdatensatz höhere Werte auf. Auch hier zeigt sich, dass durch geeignete Vorverarbeitungsmaßnahmen eine bessere Performance während des Trainings erreicht werden kann.

Deutlich wird diese Schlussfolgerung auch anhand der Metriken in Tabelle \ref{table:3.ExperimentErgebnisse}. Dort hat das Referenzmodell einen deutlich höheren Dice-Koeffizienten und auch einen höheren Jaccard-Index. Der Unterschied ist in diesem Fall nicht nur ein kleiner Prozentsatz wie in den vorherigen Experimenten, sondern ca. $7\%$, was einen deutlichen Unterschied in der Leistung darstellt.



\section{Fehleranalyse und Verbesserungen}
Im Hinblick auf die Weiterentwicklung des Modells gilt es, potentielle Fehlerquellen zu beseitigen und Verbesserungen der Modellleistung zu erarbeiten. Die Tatsache, dass jedes Modell einen gewissen ``Grundverlust'' aufweist, deutet darauf hin, dass es noch Raum für Verbesserungen gibt. Ein wichtiger Aspekt, der bereits im Kapitel \ref{sec:Datensatz} behandelt wurde, ist die Vorverarbeitung der Daten. Verschiedene Faktoren wie Skalierung, Normalisierung und Datenaugmentierung können die Leistung des Modells beeinflussen.\\
Die Ergebnisse des dritten Experiments in Abschnitt \ref{subsec:3.Experiment} zeigen, wie wichtig es ist, irrelevante Informationen aus den Bildern zu entfernen. Die in dieser Arbeit verwendeten Vorverarbeitungsmethoden könnten weiter verbessert werden, indem die Daten besser zugeschnitten werden, um möglichst viele relevante Informationen in das neuronale Netz einzuspeisen. Außerdem sollte eine höhere Auflösung der Bilder in Betracht gezogen werden, da die Skalierung zu Qualitätsverlusten führt und das Modell durch weniger verfügbare Pixel beeinträchtigt wird.
In den Experimenten wurden nur 25.000 Bilder verwendet. Eine deutliche Erhöhung dieser Anzahl würde es dem Modell ermöglichen, mehr Bilder für das Training zu verwenden und somit mehr Strukturen zu erlernen. Aufgrund begrenzter Ressourcen und zeitlicher Beschränkungen wurde jedoch nur eine Teilmenge der verfügbaren Daten verwendet. Wird die gesamte Datenmenge von ca. 160.000 Bildern für das Training verwendet, sollte auch die Anzahl der Schichten im neuronalen Netz erhöht werden. Durch eine größere Anzahl von Schichten kann das Netz mehr Merkmalskarten erzeugen, die für die Segmentierung von Hirntumoren verwendet werden können. \cite[vgl.][]{Teoh2023}\\
Eine Verbesserung der Modellleistung kann auch durch eine optimierte Wahl der Hyperparameter erreicht werden. Dies kann nicht nur zu einer Leistungssteigerung, sondern auch zu einem effektiveren Training führen. kann auch zu einer Leistungssteigerung des Modells sowie zu einem effektiveren Training führen. Dies kann durch die systematische Auswahl verschiedener Kombinationen von Hyperparametern erreicht werden. 


