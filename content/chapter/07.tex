%!TEX root = ../../main.tex
\section{Modellentwicklung}
Die Entwicklung des \gls{Modell}s ist ein anspruchsvolles Unterfangen, welches im nachfolgenden Kapitel genauer erläutert wird. Zunächst wird die Architektur des \gls{Modell}s vorgestellt und wie dieses aufgebaut ist. Der nächste Punkt erklärt was Hyperparameter sind und was diese bewirken, gefolgt von den Problemen bei der Entwicklung, sowie die Implementierung mit PyTorch. Zum Schluss wird noch der Prozess des Trainings beschrieben.
\subsection{Architektur des Neuronalen Netzes}
\label{sec:Modellarchitektur}
Die gewählte Architektur ist ein U-Net, das seinen Namen aufgrund seines U-förmigen Aufbaus (siehe Abb. \ref{fig:unet_aufbau}) bekommen hat. Das U-Net wurde speziell für die Segmentierung biomedizinischer Bilder entwickelt und ist eine spezielle Form von \ac{CNN}s. Das U-Net ist speziell darauf ausgelegt auch mit kleinen Datensätzen gute Ergebnisse zu erzielen. In der Medizin stehen oft nur begrenzt beschriftete Daten zur Verfügung, da der Aufwand für die Beschriftung der Daten hoch ist. Aufgrund dieser Tatsache ist das U-Net für diese Zwecke besonders geeignet. Die folgende Beschreibung der Architektur basiert auf dem originalen U-Net und wird in dieser Arbeit als Basis für das \gls{Modell} verwendet, welches genauer in Abschnitt \ref{subsec:Implementierung} beschrieben wird.

\begin{figure}
	\centering
	\includegraphics[width=.9\textwidth]{unet_aufbau.png}
	\caption{Aufbau eines U-Nets mit einem Encoder und Decoder Abschnitt für die Extrahierung von Merkmalen und die Wiederherstellung des Original Bildes mit der fertigen Segmentierung (Quelle: \cite{Ronneberger2015})}
	\label{fig:unet_aufbau}
\end{figure}
Die Architektur des Netzes besteht aus zwei Teilen, dem Downsampling Pfad, auch Encoder genannt, und dem Upsampling Pfad, der Decoder. Der Encoder ähnelt einem klassischen \ac{CNN} und besteht aus einer reihe von Convolutional und Max-Pooling Operationen, wobei es jedoch keine vollständig vernetzen Schichten gibt. 

Der Encoder wird verwenden, um die Feature-Maps aus dem Bild zu extrahieren. Bei jedem Schritt, den die Eingabe durch den Encoder macht, werden mehr Feature-Maps extrahiert und die räumliche Dimension des Bildes verringert. Die Ausgabe wird an die nächsthöhere Schicht weitergegeben, dies dient dazu möglichst viele Details auf verschiedenen Ebenen zu erhalten. Der Aufbau besteht dabei aus einer Reihe von zweifach Faltungen mit einem 3x3 Filter, auf die jeweils immer eine Max-Pooling Schicht mit einer Filtergröße von 2x2 und einer Schrittweite von 2 folgt. Bei jedem Encoder Schritt wird die Anzahl der Filter, bzw. Feature-Maps in den Faltungsschichten verdoppelt.

Im Decoder wird die komprimierte Eingabe wieder auf die ursprüngliche Bildgröße skaliert, auch ''Upsampling`` genannt. Dies geschieht durch eine Aneinanderreihung von Upsampling Schichten jeweils gefolgt von zweifach Faltungen und einer Aktivierungsfunktion. Nach jedem Upsampling der Feature-Maps folgt eine Faltung mit einem 2x2 Filter, welche die Anzahl der Feature-Maps jeweils halbiert. Hierbei findet zusätzlich eine Verkettung der Feature-Maps aus dem entsprechenden Encoder Abschnitt statt. Nach der 2x2-Faltung folgen jeweils immer zwei 3x3-Faltungen mit einer abschließenden ReLU Aktivierungsfunktion. Abschließend an den Decoder Abschnitt folgt noch eine Faltung mit einem 1x1 Filter, der die Features auf die gewünschte Anzahl von Klassen der Segmentierungsaufgabe abbildet. \cite[vgl.][]{Ronneberger2015}

\subsection{Hyperparameter}
Hyperparameter sind Parameter, deren Werte vor dem Training eines \gls{Modell}s festgelegt und während des Trainings, bis auf gewisse Ausnahmen nicht angepasst werden. Sie sind entscheidend für die Leistung des Modells und müssen sorgfältig ausgewählt werden. Die Auswahl der richtigen Hyperparameter ist oft eine Kunst und erfordert sowohl Erfahrung als auch experimentelles Ausprobieren. 

In der Praxis wird häufig eine Technik namens Hyperparametersuche oder -optimierung verwendet, bei der verschiedene Kombinationen von Hyperparametern systematisch ausprobiert werden, um diejenige zu finden, die die beste Leistung liefert. In diesem Abschnitt werden die wichtigsten Hyperparameter, die bei der Entwicklung des eines Deep Learning \glspl{Modell} relevant sind genauer beschrieben.

\paragraph{Lernrate} Die Lernrate ist ein entscheidender Hyperparameter in fast allen Optimierungsalgorithmen für Neuronale Netze. Sie bestimmt, wie stark die Gewichte im Netzwerk in jedem Schritt des Trainings angepasst werden. Eine zu hohe Lernrate kann dazu führen, dass das globale Minimum der Verlustfunktion übersprungen wird und das \gls{Modell} möglicherweise nicht konvergiert. Bei einer zu niedrigen Lernrate, wird der Fehler immer nur um sehr kleine Werte verringert, was zu einer langsamen Konvergenz führt. Es kann auch passieren, das der Fehler um einen Wert oszilliert und damit bei einer suboptimalen Lösung steckenbleibt. \cite[vgl.][]{Pfannstiel2022}

\paragraph{Batch Größe} Die Batch Größe bezieht sich auf die Anzahl der Trainingsbeispiele, die das Netzwerk gleichzeitig sieht, bevor es seine Gewichte aktualisiert. Eine größere Batch Größe kann zu stabileren Aktualisierungen der Gewichte führen, aber benötigt mehr Speicherplatz und verlangsamt das Training. Eine kleinere Batch Größe hingegen kann zu einem schnelleren, aber weniger stabilen Training führen. Übliche Größen für eine Batch reichen von 2, 8, 16 bis hin zu hunderten von Bildern in einer Batch. \cite[vgl.][]{Yu2020}

\paragraph{Anzahl der Epochen} Eine Epoche bezeichnet einen Durchlauf des gesamten Datensatzes während des Trainings. Die Anzahl der Epochen, für die das \gls{Modell} trainiert wird, beeinflusst, wie gut es Muster aus den Daten erlernen kann. Es ist wichtig eine geeignete Anzahl von Epochen auszuwählen, um einen optimalen Trainingsprozess zu erhalten. Bei einer zu niedrigen Anzahl an Epochen, hat das Netz nicht genügend Zeit, wichtige Muster zu lernen, wobei es zum Underfitting kommen kann. Ist die Anzahl der Epochen hingegen zu hoch, besteht die Gefahr, dass das Netz sich zu sehr an die Trainingsdaten anpasst und es zum Overfitting kommt. \cite[vgl.][]{Goodfellow2016}

\paragraph{Architektur spezifische Parameter} Im Kontext der U-Net Architektur gibt es auch spezifische Hyperparameter, die bei der Entwicklung des \glspl{Modell} berücksichtigt werden müssen, wie die Anzahl und Größe der Filter in den Faltungsschichten, sowie die Tiefe des Netzes und die Art der Aktivierungsfunktionen.


\subsection{Implementierung}
\label{subsec:Implementierung}
Die Implementierung des \gls{Modell}s für die Segmentierung der Gehirntumore erfolgt in Python mit PyTorch und basiert auf der U-Net Architektur, welche in \ref{sec:Modellarchitektur} erläutert wurde. Die Implementierung weicht in geringen Teilen von der originalen Architektur des U-Net ab. Die grobe Struktur des U-Net wurde weitestgehend übernommen, mit jeweils vier Down- und Upsampling Schichten. 

\subsubsection{Padding}
Aufgrund der Faltungen innerhalb des Netzes reduziert sich die Bildgröße, da keine Fülldaten verwendet werden. Um dem entgegenzuwirken, aus Gründen der besseren Implementierung, wird an jeden Rand des Bildes eine Bestimmte Anzahl von Pixeln hinzugefügt. Da die Bildgröße bekannt ist aufgrund der gegebenen Architektur (siehe Abschnitt \ref{paragraph:GrößenSkalierung}), kann für die Berechnung der Füllmenge $P$ die Formel \ref{eq:conv_size} verwendet werden. Die Bildgröße $W$ ist definiert durch ein vielfaches von $2^N$, wobei $N$ die Anzahl der Downsampling Schichten beschreibt. Im konkreten Fall ist $N=4$ und die Eingabegröße der Bilder ist 128x128 Pixel, was ein vielfaches von $2^N=2^4=16$ ist. Die Schrittweite $S$ und Filtergröße $F$ sind bereits durch den Aufbau des U-Net mit $S=1$ und $F=3$ gegeben. Daraus ergibt sich die Füllmenge mit Formel \ref{eq:conv_size}:
\begin{equation}
	P = \dfrac{W \times S - 1 - W + F}{2}
\end{equation}

\begin{equation}
	P = \dfrac{2^N \times 1 - 1 - 2^N + 3}{2}  = 1
\end{equation}
Durch das Beibehalten der Eingabegröße, ist die Implementierung aufgrund der einfachen Berechnung der Bildgröße für jede Schicht sehr von Vorteil. 

\subsubsection{Batch Normalisierung}
Die Batch Normalisierung ist eine Technik, die häufig beim Deep Learning verwendet wird, um das Training zu beschleunigen und die Stabilität des Netzwerks zu erhöhen. Das Vorgehen bei der Batch Normalisierung ist es, die Eingaben zu jeder Schicht zu normalisieren, indem die mittlere Aktivierung auf 0 und die Standardabweichung der Aktivierung auf 1 gesetzt wird.\\
Hierfür wird in der Regel der Mittelwert und die Standardabweichung für jedes Batch berechnet, daher auch der Name Batch Normalisierung. Es zeigte sich, dass durch Batch Normalisierung höhere Lernraten verwendet werden können und das Training im Allgemeinen schneller und effektiver verläuft. Zusätzlich wird die Stabilität des Netzwerks erhöht und die Notwendigkeit für sorgfältige Initialisierung der Modellparameter entfällt. Die Schicht wirkt außerdem teilweise als eine Art Dropout, bei welchem das Modell bestimmte Neuronen ``vergisst'' und neu initialisiert, um so ein Overfitting zu vermeiden.\cite[vgl.][]{Ioffe2015}
PyTorch stellt bereits eine Batch Normalisierungs Schicht zur Verfügung, die für die Normalisierung der Batches verwendet werden kann. Diese Schicht wird nach jeder Faltung und noch vor der Aktivierungsfunktion angewendet.

\subsection{Verlustfunktion}
Die Auswahl der richtigen Verlustfunktion ist entscheidend für den Erfolg eines Neuronalen Netz. Bei der Klassifizierung von medizinischen Bilddaten, gibt es einige Herausforderungen, die eine spezielle Auswahl der Verlustfunktion erfordern.\\
Die Kreuzentropie (engl.: Cross Entropy) ist eine gängige Wahl für die Verlustfunktion bei Klassifizierungsproblemen. Sie misst, wie gut die geschätzte Wahrscheinlichkeitsverteilung des Modells mit der tatsächlichen Verteilung der Daten übereinstimmt. Wenn die Vorhersage des Modells genau mit den tatsächlichen Klassen übereinstimmt, ist der Cross-Entropy-Verlust Null. Wenn die Vorhersage jedoch weit von der tatsächlichen Klasse entfernt ist, steigt der Verlust exponentiell an. \cite[vgl.][]{Murphy2012}\\
Obwohl die Kreuzentropie in den meisten Fällen recht effektiv ist, kommt diese insbesondere bei der Segmentierung von medizinischen Bildern an ihre Grenzen. Eines der Hauptprobleme ist, dass die Kreuzentropie Pixelweise berechnet wird und somit kein globales Verständnis für die räumliche Struktur des Bildes hat. Bei der Segmentierung von Gehirntumoren ist es jedoch wichtig auch benachbarte Pixel zu betrachten, um so die räumliche Struktur im Blick zu behalten. \\
Ein anderes Problem ist das Ungleichgewicht der verschiedenen Klassen. Die zu segmentierenden Tumore sind meist wesentlich kleiner als das gesamte Bild, was zur ungleichen Verteilung von Tumor- und Hintergrundklassen führt. Aufgrund dessen ist die Kreuzentropie stark beeinflusst durch die Hintergrundelement und fällt damit schnell gegen Null. \cite[][]{Yeung2021}\\
Aus diesen Gründen wird häufig eine weitere Verlustfunktion hinzugezogen. Eine der verbreitetsten Verlustfunktionen für die Segmentierung ist der Dice Loss. Dieser ist eine regionsabhängige Verlustfunktion, welcher auch die räumliche Struktur des Bildes für die Minimierung des Fehlers berücksichtigt. Der Dice Loss basiert auf dem Dice-Koeffizienten oder der Sørensen-Dice-Ähnlichkeit, einer Metrik zur quantitativen Bewertung der Ähnlichkeit zwischen zwei Mengen. Der Dice-Koeffizient wird definiert als das Verhältnis des doppelten Schnitts der beiden Mengen zum Gesamtvolumen der Mengen
\begin{equation}
	DSC={\frac {2|X\cap Y|}{|X|+|Y|}}
\end{equation}
Betrachtet man den Dice-Koeffizient mit boolschen Daten, also nur mit richtig oder falsch, so kann dieser auch mit 
\begin{equation}
	DSC={\frac {2TP}{2TP+FP+FN}}
\end{equation}
dargestellt werden. Im Kontext der Bildsegmentierung wird der Dice-Koeffizient verwendet, um die Ähnlichkeit zwischen der vorhergesagten Segmentierung und der tatsächlichen Ground-Truth-Segmentierung zu messen. \cite[vgl.][]{Sudre2017}



\subsection{Probleme bei der Entwicklung}
Bei der Entwicklung von Deep Learning \gls{Modell}en für medizinische Bildsegmentierung können unterschiedliche Probleme auftreten, die durch verschiedene Faktoren verursacht werden. Eines der häufigsten Probleme bei der Entwicklung ist der Mangel an Grafikkartenspeicher. Aufgrund der  Bildgröße der \ac{MRT} Scans passen relativ wenige Bilder in den Speicher der Grafikkarte. Behält man die Dimension der ursprünglichen Bilder bei, so würde nur eine sehr geringe Größe für eine Batch funktionieren, ohne den Speicher zu überfüllen. \\
Die Bilder wurden daher wie in Abschnitt \ref{subsec:Vorverarbeitung} beschrieben, in 2D Bilder konvertiert und auf eine kleinere Bildgröße skaliert. Aufgrund der Konvertierung von 3D zu 2D kann nun eine deutlich Größe Anzahl an Bildern pro Batch verwendet werden, um das \gls{Modell} zu trainieren.\\
Ein weiteres Problem, ist das Fehlen von leistungsstarker Hardware für schnellere Berechnungen. Die Hardware welche für das Training von Deep Learning \gls{Modell}en verwendet wird ist häufig sehr kostenintensiv und benötigt eine Menge Strom. Die verwendete Grafikkarte bei der Entwicklung ist daher lediglich eine Consumer Grafikkarte, welche für das Training von vergleichsweise kleinen \gls{Modell}en geeignet ist.

\subsection{Training}
Bevor ein Neuronales Netz trainiert werden kann, muss gewährleistet sein, dass die Daten ausreichend vorverarbeitet wurden und somit geeignet sind für das Training. Sind die Daten in einer geeigneten Form, kann das Training beginnen. Das Training eines Netzes verläuft über mehrere Epochen und kann eine Menge Zeit in Anspruch nehmen. Bei jeder Epoche werden alle Bilder des Trainingsdatensatz einmal durch das Netzwerk gegeben und anschließend die Leistung mittels eines Validierungsdatensatzes gemessen. Dieser Prozess wird für eine vorher ausgewählte Anzahl an Epochen durchgeführt.\\
Der Prozess beginnt mit der Initialisierung des \gls{Modell}s, bei welchem alle internen Parameter auf einen zufälligen Wert gesetzt und im Verlauf des Trainings angepasst werden. Anschließend werden die Trainingsdaten in Form von \glspl{Batch} durch das Netzwerk gereicht, auch Feedforward genannt. Bei diesem Teil, werden verschiedenen Berechnungen innerhalb des Netzes ausgeführt, um eine Vorhersage bzw. Segmentierung für die Daten zu erstellen.\\
Nachdem eine \gls{Batch} durch das Netzwerk gereicht wurde und die Segmentierungen erstellt wurden, wird der Fehler berechnet. Dieser sagt aus, wie weit die Ausgaben des Netzes mit dem tatsächlichen Ergebnissen auseinander liegen. Anhand dieses Fehlers werden dann im nächsten Schritt die Parameter innerhalb des \gls{Modell}s angepasst. Mithilfe der Rückwärtspropagierung werden die Fehler eines jeden Neurons zum Gesamtfehler berechnet. Anhand dessen können die Gewichte und Bias eines jeden Neuron optimiert werden. \cite[vgl.][]{Goodfellow2016}