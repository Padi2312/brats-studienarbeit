@Book{Ramsundar2020,
  author    = {Ramsundar, Bharath and Eastman, Peter and Walters, Patrick and Pande, Vijay},
  date      = {2020-02-20},
  title     = {Deep Learning für die Biowissenschaften},
  isbn      = {3960091303},
  pages     = {1},
  publisher = {Dpunkt.Verlag GmbH},
  url       = {https://www.ebook.de/de/product/38103997/bharath_ramsundar_peter_eastman_patrick_walters_vijay_pande_deep_learning_fuer_die_biowissenschaften.html},
  ean       = {9783960091301},
  ranking   = {rank1},
  year      = {2020},
}

@Book{Posthoff2022,
  author    = {Posthoff, Christian},
  date      = {2022},
  title     = {Computer und Künstliche Intelligenz},
  isbn      = {9783658377670},
  pages     = {9f,74f},
  pagetotal = {243},
  publisher = {Springer Fachmedien Wiesbaden GmbH},
  subtitle  = {Vergangenheit - Gegenwart - Zukunft},
}

@WWW{WasIstKi,
  author  = {Brockhaus Enzyklopädie Online},
  title   = {Künstliche Intelligenz},
  url     = {https://brockhaus.de/ecs/enzy/article/kunstliche-intelligenz},
  urldate = {2023-02-20},
}

@Book{Lang2023,
  author    = {Lang, Volker},
  date      = {2023-01-18},
  title     = {Digitale Kompetenz},
  pages     = {178f,198f},
  pagetotal = {316},
  publisher = {Springer Berlin Heidelberg},
  url       = {https://www.ebook.de/de/product/45515417/volker_lang_digitale_kompetenz.html},
  ean       = {9783662662854},
  year      = {2023},
}

@Book{Ertel2021,
  author    = {Ertel, Wolfgang},
  date      = {2021-09-23},
  title     = {Grundkurs Künstliche Intelligenz},
  pages     = {288,351,325},
  pagetotal = {423},
  publisher = {Springer Fachmedien Wiesbaden},
  url       = {https://www.ebook.de/de/product/41631013/wolfgang_ertel_grundkurs_kuenstliche_intelligenz.html},
  ean       = {9783658320751},
  year      = {2021},
}

@Book{Alpaydin2014,
  author    = {Alpaydin, Ethem},
  date      = {2014},
  title     = {Introduction to Machine Learning},
  isbn      = {9780262028189},
  pages     = {1f},
  pagetotal = {613},
  publisher = {The MIT Press},
}

@WWW{Algorithmus,
  author  = {o.V.},
  date    = {2022-01-16},
  title   = {Was ist ein Algorithmus?},
  url     = {https://www.itwissen.info/Algorithmus-algorithm.html},
  urldate = {2023-02-22},
}

@WWW{MSModell,
  author  = {o.V.},
  date    = {12-30-2021},
  title   = {Was ist ein Machine Learning Modell?},
  url     = {https://learn.microsoft.com/en-us/windows/ai/windows-ml/what-is-a-machine-learning-model},
  urldate = {2023-02-26},
}

@WWW{IBMSupervisedLearning,
  author  = {o.V.},
  date    = {o.J.},
  title   = {Supervised Learning},
  url     = {https://www.ibm.com/de-de/topics/supervised-learning},
  urldate = {2023-02-26},
}

@misc{GoogleMusicLM,
  doi = {10.48550/ARXIV.2301.11325},
  url = {https://arxiv.org/abs/2301.11325},
  author = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zalán and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
  keywords = {Sound (cs.SD), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {MusicLM: Generating Music From Text},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Book{Frochte2020,
  author    = {Jörg Frochte},
  date      = {2020},
  title     = {Maschinelles Lernen},
  isbn      = {9783446461444},
  pages     = {21ff,24,180},
  pagetotal = {608},
  publisher = {Hanser, Carl GmbH + Co.},
  url       = {https://www.ebook.de/de/product/39878236/joerg_frochte_maschinelles_lernen.html},
  ean       = {9783446463554},
  year      = {2020},
}

@Book{JuergenCleve2020,
  author    = {Jürgen Cleve, Uwe Lämmel},
  date      = {2020-03-09},
  title     = {Künstliche Intelligenz},
  pages     = {190ff},
  pagetotal = {336},
  publisher = {Hanser, Carl GmbH + Co.},
  url       = {https://www.ebook.de/de/product/38679436/juergen_cleve_uwe_laemmel_kuenstliche_intelligenz.html},
  ean       = {9783446463639},
  year      = {2020},
}

@TechReport{Rosenblatt1958,
  author    = {Frank Rosenblatt},
  date      = {1958},
  title     = {The perceptron: A probabilistic model for information storage and organization in the brain.},
  publisher = {Psychological Review 65},
}

@Book{Goodfellow2016,
  author    = {Ian Goodfellow, Yoshua Bengio, Aaron Courville},
  title     = {Deep Learning},
  note      = {\url{http://www.deeplearningbook.org}},
  publisher = {MIT Press},
  year      = {2016},
}

@Book{Scherer1997,
  author    = {Andreas Scherer},
  date      = {1997},
  title     = {Neuronale Netze Grundlagen und Anwendungen},
  isbn      = {9783322868305},
  pages     = {65f},
  pagetotal = {233},
  publisher = {Vieweg+Teubner Verlag},
  subtitle  = {Grundlagen und Anwendungen},
}

@WWW{ChristophPabst2013,
  author     = {Dr. med. Christoph Pabst},
  date       = {2013},
  title      = {Magnetresonanz-Tomographie},
  url        = {https://www.ukgm.de/ugm_2/deu/umr_rdi/Teaser/Grundlagen_der_Magnetresonanztomographie_MRT_2013.pdf},
  subtitle   = {Lernskript für Mediziner},
  titleaddon = {Grundlagen der Magnetresonanz-Tomographie},
  urldate    = {2023-03-13},
}

@Book{Kramme2016,
  author    = {Kramme, Rüdiger},
  date      = {2016},
  title     = {Medizintechnik Verfahren - Systeme - Informationsverarbeitung},
  isbn      = {9783662487716},
  pages     = {330},
  publisher = {Springer Berlin / Heidelberg},
  subtitle  = {Verfahren - Systeme - Informationsverarbeitung},
}

@WWW{IonisierendeStrahlung,
  author  = {o.V.},
  date    = {2020-06-05},
  title   = {Ionisierende Strahlung},
  url     = {https://www.bmuv.de/themen/atomenergie-strahlenschutz/strahlenschutz/ionisierende-strahlung},
  urldate = {2023-03-13},
}

@WWW{Kernspin2022,
  author  = {Dr. Frank Antwerpes},
  date    = {2022-11-05},
  title   = {Kernspintomographie},
  url     = {https://flexikon.doccheck.com/de/Kernspintomographie},
  urldate = {2023-03-13},
}

@Book{Heinrichs2022,
  author    = {Heinrichs, Jan-Hendrik and Caspers, Svenja and Schnitzler, Alfons and Seitz, Frederike},
  date      = {2022-12-09},
  title     = {Bildgebung in den Neurowissenschaften},
  pagetotal = {170},
  publisher = {Verlag Karl Alber},
  url       = {https://www.ebook.de/de/product/45689204/jan_hendrik_heinrichs_svenja_caspers_alfons_schnitzler_frederike_seitz_bildgebung_in_den_neurowissenschaften.html},
  ean       = {9783495997918},
  year      = {2022},
}

@WWW{PyTorch,
  author  = {o.V.},
  title   = {PyTorch},
  url     = {https://pytorch.org/},
  urldate = {2023-04-02},
}

@Book{PapaJoe,
  author    = {Papa, Joe},
  title     = {PyTorch kompakt : Syntax, Design Patterns und Codebeispiele für Deep-Learning-Modelle},
  isbn      = {9783960091851},
  publisher = {Dpunkt.Verlag GmbH},
  subtitle  = {Syntax, Design Patterns und Codebeispiele für Deep-Learning-Modelle},
}

@Book{Teoh2023,
  author    = {Teoh, Teik Toe},
  date      = {2023},
  title     = {Convolutional Neural Networks for Medical Applications},
  isbn      = {9789811988134},
  publisher = {Springer},
}

@Book{Weidman2020,
  author    = {Weidman, Seth},
  date      = {2020},
  title     = {Deep Learning - Grundlagen und Implementierung : Neuronale Netze mit Python und PyTorch programmieren},
  isbn      = {9783960091363},
  publisher = {Dpunkt.Verlag GmbH},
  subtitle  = {Neuronale Netze mit Python und PyTorch programmieren},
}

@Book{Frick2021,
  author    = {Frick, Detlev},
  date      = {2021},
  title     = {Data Science Konzepte, Erfahrungen, Fallstudien und Praxis},
  isbn      = {9783658334024},
  pages     = {402},
  publisher = {Springer Fachmedien Wiesbaden GmbH},
  subtitle  = {Konzepte, Erfahrungen, Fallstudien und Praxis},
}

@Misc{Kingma2014,
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  date      = {2014},
  title     = {Adam: A Method for Stochastic Optimization},
  doi       = {10.48550/ARXIV.1412.6980},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Online{RSNABrainTumor2021,
  author  = {o.V.},
  date    = {2021},
  title   = {Brain Tumor AI Challenge (2021)},
  url     = {https://www.rsna.org/education/ai-resources-and-training/ai-image-challenge/brain-tumor-ai-challenge-2021},
  urldate = {2023-04-24},
}

@InProceedings{SwiebockaWiek2016,
  author    = {Swiebocka-Wiek, Joanna},
  booktitle = {Computer Information Systems and Industrial Management},
  title     = {Skull Stripping for MRI Images Using Morphological Operators},
  editor    = {Saeed, Khalidand Homenda, W{\l}adys{\l}aw},
  isbn      = {978-3-319-45378-1},
  publisher = {Springer International Publishing},
  abstract  = {One of the most common MRI (Magnetic Resonance Imaging) use is a brain visualisation. Brain anatomy is highly complicated therefore it might be difficult to extract only these structures which have diagnostic value. In a consequence it is so necessary to develop and apply most efficient brain's segmentation algorithms. One of the first steps in case of neurological MRI analysis is skull stripping. It involves removing extra-meningeal tissue from the head image, therefore it is essential to find the best method to determine the brain and skull boundaries. In T1-weighted images, cerebrospinal fluid (CSF) space and skull are dark, that is why the edges between the brain and the skull are well-marked but even strong edges might be unsettled because of finite resolution during MRI acquisition or the presence of other anatomical partial structures within the brain (connections between the brain and optic nerves or brainstem). There are many ways to perform this operation, none of them is not so great as to constitute a standard proceedings. In many cases, there are limitations associated with the development environment, license and images input that hinder skull stripping without specialised software. Proposed method is free of these constraints. It is based on application of morphological operations and image filtration to enhance the result of the edge detection and to provide better tissues separation. The efficiency was compared with other methods, common in commercial use, and the results of this comparison was presented in this paper.},
  address   = {Cham},
  year      = {2016},
}

@Article{Baid2021,
  author      = {Baid, Ujjwal and Ghodasara, Satyam and Mohan, Suyash and Bilello, Michel and Calabrese, Evan and Colak, Errol and Farahani, Keyvan and Kalpathy-Cramer, Jayashree and Kitamura, Felipe C. and Pati, Sarthak and Prevedello, Luciano M. and Rudie, Jeffrey D. and Sako, Chiharu and Shinohara, Russell T. and Bergquist, Timothy and Chai, Rong and Eddy, James and Elliott, Julia and Reade, Walter and Schaffter, Thomas and Yu, Thomas and Zheng, Jiaxin and Moawad, Ahmed W. and Coelho, Luiz Otavio and McDonnell, Olivia and Miller, Elka and Moron, Fanny E. and Oswood, Mark C. and Shih, Robert Y. and Siakallis, Loizos and Bronstein, Yulia and Mason, James R. and Miller, Anthony F. and Choudhary, Gagandeep and Agarwal, Aanchal and Besada, Cristina H. and Derakhshan, Jamal J. and Diogo, Mariana C. and Do-Dai, Daniel D. and Farage, Luciano and Go, John L. and Hadi, Mohiuddin and Hill, Virginia B. and Iv, Michael and Joyner, David and Lincoln, Christie and Lotan, Eyal and Miyakoshi, Asako and Sanchez-Montano, Mariana and Nath, Jaya and Nguyen, Xuan V. and Nicolas-Jilwan, Manal and Jimenez, Johanna Ortiz and Ozturk, Kerem and Petrovic, Bojan D. and Shah, Chintan and Shah, Lubdha M. and Sharma, Manas and Simsek, Onur and Singh, Achint K. and Soman, Salil and Statsevych, Volodymyr and Weinberg, Brent D. and Young, Robert J. and Ikuta, Ichiro and Agarwal, Amit K. and Cambron, Sword C. and Silbergleit, Richard and Dusoi, Alexandru and Postma, Alida A. and Letourneau-Guillon, Laurent and Perez-Carrillo, Gloria J. Guzman and Saha, Atin and Soni, Neetu and Zaharchuk, Greg and Zohrabian, Vahe M. and Chen, Yingming and Cekic, Milos M. and Rahman, Akm and Small, Juan E. and Sethi, Varun and Davatzikos, Christos and Mongan, John and Hess, Christopher and Cha, Soonmee and Villanueva-Meyer, Javier and Freymann, John B. and Kirby, Justin S. and Wiestler, Benedikt and Crivellaro, Priscila and Colen, Rivka R. and Kotrotsou, Aikaterini and Marcus, Daniel and Milchenko, Mikhail and Nazeri, Arash and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre and Mahajan, Abhishek and Menze, Bjoern and Flanders, Adam E. and Bakas, Spyridon},
  date        = {2021-07-05},
  title       = {The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification},
  doi         = {10.48550/ARXIV.2107.02314},
  eprint      = {2107.02314},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {The BraTS 2021 challenge celebrates its 10th anniversary and is jointly organized by the Radiological Society of North America (RSNA), the American Society of Neuroradiology (ASNR), and the Medical Image Computing and Computer Assisted Interventions (MICCAI) society. Since its inception, BraTS has been focusing on being a common benchmarking venue for brain glioma segmentation algorithms, with well-curated multi-institutional multi-parametric magnetic resonance imaging (mpMRI) data. Gliomas are the most common primary malignancies of the central nervous system, with varying degrees of aggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets the evaluation of computational algorithms assessing the same tumor compartmentalization, as well as the underlying tumor's molecular characterization, in pre-operative baseline mpMRI data from 2,040 patients. Specifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation of the histologically distinct brain tumor sub-regions, and b) the classification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT) promoter methylation status. The performance evaluation of all participating algorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse platform (Task 1) and Kaggle (Task 2), concluding in distributing to the top ranked participants monetary awards of $60,000 collectively.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:http\://arxiv.org/pdf/2107.02314v2:PDF},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2021},
}

@Article{Gizewski2001,
  author       = {E. RM. R Gizewski},
  date         = {2001},
  journaltitle = {{RöFo} - Fortschritte auf dem Gebiet der Röntgenstrahlen und der bildgebenden Verfahren},
  title        = {Epidermoid oder Arachnoidalzyste: {CISS}, {FLAIR} und Diffusionsbilder als Ausweg aus dem diagnostischen Dilemma},
  doi          = {10.1055/s-2001-10232},
  number       = {1},
  pages        = {77--78},
  volume       = {173},
  publisher    = {Georg Thieme Verlag {KG}},
}

@Article{Nam2021,
  author       = {Yeo Kyung Nam and Ji Eun Park and Seo Young Park and Minkyoung Lee and Minjae Kim and Soo Jung Nam and Ho Sung Kim},
  date         = {2021-08},
  journaltitle = {European Radiology},
  title        = {Reproducible imaging-based prediction of molecular subtype and risk stratification of gliomas across different experience levels using a structured reporting system},
  doi          = {10.1007/s00330-021-08015-4},
  number       = {10},
  pages        = {7374--7385},
  volume       = {31},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Online{NIfTI,
  author  = {Data Format Working Group},
  date    = {2013-12},
  title   = {NIfTI (Neuroimaging Informatics Technology Initiative)},
  url     = {https://nifti.nimh.nih.gov/},
  urldate = {2023-04-27},
}

@Article{Ronneberger2015,
  author      = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date        = {2015-05-18},
  title       = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  doi         = {10.48550/ARXIV.1505.04597},
  eprint      = {1505.04597},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:http\://arxiv.org/pdf/1505.04597v1:PDF},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2015},
}

@Article{Shorten2019,
  author       = {Connor Shorten and Taghi M. Khoshgoftaar},
  date         = {2019-07},
  journaltitle = {Journal of Big Data},
  title        = {A survey on Image Data Augmentation for Deep Learning},
  doi          = {10.1186/s40537-019-0197-0},
  number       = {1},
  volume       = {6},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Book{Stevens2020,
  author    = {Stevens, Eli Eli and Antiga, Luca Luca and Viehmann, Thomas Thomas},
  date      = {2020},
  title     = {Deep Learning with Pytorch},
  isbn      = {9781617295263},
  pages     = {520},
  publisher = {Manning Publications Company},
}

@Book{Choo2020,
  author    = {Kenny Choo and Eliska Greplova and Mark H. Fischer and Titus Neupert},
  date      = {2020},
  title     = {Machine Learning kompakt},
  doi       = {10.1007/978-3-658-32268-7},
  publisher = {Springer Fachmedien Wiesbaden},
}

@Article{Rumelhart1986,
  author       = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  date         = {1986-10},
  journaltitle = {Nature},
  title        = {Learning representations by back-propagating errors},
  doi          = {10.1038/323533a0},
  number       = {6088},
  pages        = {533--536},
  volume       = {323},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Book{Pfannstiel2022,
  date      = {2022},
  title     = {Künstliche Intelligenz im Gesundheitswesen},
  doi       = {10.1007/978-3-658-33597-7},
  editor    = {Mario A. Pfannstiel},
  publisher = {Springer Fachmedien Wiesbaden},
}

@Misc{Bengio2012,
  author    = {Bengio, Yoshua},
  date      = {2012},
  title     = {Practical recommendations for gradient-based training of deep architectures},
  doi       = {10.48550/ARXIV.1206.5533},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Yu2020,
  author      = {Yu, Tong and Zhu, Hong},
  date        = {2020-03-12},
  title       = {Hyper-Parameter Optimization: A Review of Algorithms and Applications},
  doi         = {10.48550/ARXIV.2003.05689},
  eprint      = {2003.05689},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:http\://arxiv.org/pdf/2003.05689v1:PDF},
  keywords    = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2020},
}

@Article{Ioffe2015,
  author      = {Ioffe, Sergey and Szegedy, Christian},
  date        = {2015-02-11},
  title       = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  doi         = {10.48550/ARXIV.1502.03167},
  eprint      = {1502.03167},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:http\://arxiv.org/pdf/1502.03167v3:PDF},
  keywords    = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2015},
}

@Book{Murphy2012,
  author    = {Murphy, Kevin P.},
  date      = {2012},
  title     = {Machine learning a probabilistic perspective},
  isbn      = {9780262018029},
  publisher = {MIT Press},
  subtitle  = {a probabilistic perspective},
}

@Misc{Yeung2021,
  author    = {Yeung, Michael and Sala, Evis and Schönlieb, Carola-Bibiane and Rundo, Leonardo},
  date      = {2021},
  title     = {Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation},
  doi       = {10.48550/ARXIV.2102.04525},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.6; J.3},
  publisher = {arXiv},
}

@Article{Sudre2017,
  author      = {Sudre, Carole H and Li, Wenqi and Vercauteren, Tom and Ourselin, Sébastien and Cardoso, M. Jorge},
  date        = {2017-07-11},
  title       = {Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations},
  doi         = {10.1007/978-3-319-67558-9_28},
  eprint      = {1707.03237},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  pages       = {240--248},
  abstract    = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
  booktitle   = {Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:http\://arxiv.org/pdf/1707.03237v3:PDF},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher   = {Springer International Publishing},
  year        = {2017},
}

@Comment{jabref-meta: databaseType:biblatex;}
